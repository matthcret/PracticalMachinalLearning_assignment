---
title: "Practical Machine Learning Course Project"
author: "Matth Cret"
date: "August, 12 2017"
output:
  html_document:
    keep_md: true
---

The study is carried out using RStudio 1.0.143.

## Project Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here:  
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Getting Data

The training data for this project are available here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

Both csv files are downloaded in the same directory where the R Markdown file is located. The data sets are loaded as data.frame objects set.training and set.testing respectively.

```{r gettingdata}
### Initialize the url and file name
url.training <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url.testing <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training.csv <- paste(getwd(),"/training.csv",sep = "")
testing.csv <- paste(getwd(),"/testing.csv",sep = "")

### Download the csv files
if (!file.exists(training.csv)) {
    download.file(url.training, destfile=training.csv)
}
if (!file.exists(testing.csv)) {
    download.file(url.testing, destfile=testing.csv)
}

### Load the csv files as data.frame objects
set.training <- read.csv(training.csv)
set.testing <- read.csv(testing.csv)

### Analyse dimensions of the two datasets
dim(set.training)
dim(set.testing)
```

From above it can be seen that the training dataset is way bigger than the testing dataset.

## Cleaning Data & Exploratory Analysis

First the predictors with zero variance are found and remove from the training set.

```{r cleaningdata0}
### Load caret package
library(caret)

### Identify predictors with zero variance
zerovar <- nearZeroVar(set.training, saveMetrics=FALSE)

### Count the number of predictors with zero variance
length(zerovar)
```

60 predictors have zero variance. All of them will be removed from set.training.

```{r cleaningdata1}
### Remove some predictors that have no value
set.training <- set.training[,-zerovar]
```

Now the focus go to predictors have missing values NA.

```{r cleaningdata2}
### Count the number of predictors with some missing values
length(set.training[colSums(is.na(set.training)) > 0])
```

41 predictors have missing values. All of them will be removed from set.training.

```{r cleaningdata3}
### Identify columns indexes of predictors with half of the rows with missing values
na.index <- c()
for (i in 1:ncol(set.training)) {
    if (sum(is.na(set.training[,i])) >= nrow(set.training)/2) {
        na.index <- c(na.index, i)
    }
}

### Remove some predictors with missing values
set.training <- set.training[, -na.index]

### Display first three rows for each remaining column
head(set.training,3)
```

UserIDs and timestamps are not useful predictors that can be used in order to predict the variable "classe", hence they can be removed from the dataset. These predictors correspond to the first 6 columns.

```{r cleaningdata4}
### Remove useless predictors
set.training <- set.training[, -c(1, 2, 3, 4, 5, 6)]
```

The following step is the identification of correlated predictors.

```{r exploratoryanalysis0}
### Load corrplot package
library(corrplot)

### Create a correlation matrix
cormatrix <- abs(cor(set.training[,-53]))

### NA are translated into 0s
cormatrix[is.na(cormatrix)] <- 0

### Remove the influence of predictors strongly related to themselves
diag(cormatrix) <- 0

### Plot the correlation
corrplot(cormatrix)
```

From the matrix can be seen how some of the predictors are strongly correlated to some others. This is not good for the model, whose accuracy decreases in such a case.

```{r exploratoryanalysis1}
### Identify the predictors that are highly correlated
cor <- findCorrelation(cormatrix, cutoff=0.8)

### Remove the predictors that are corellated
set.training <- set.training[, -cor]

### Analyse dimensions of the dataset
dim(set.training)
```

The dataset ends up having 40 predictors.

## Predictive Modeling

classe has 5 distict values: A, B, C, D, E. Such variables are called categorical and the problem of the study is, hence, a classification problem. Decision Tree, Random Forest, and the Gradient Boost supervised machine learning algorithms will be applied.

A good modeling approach needs a training data set and a validation data set. The data in set.training will undergo a 75/25 split of the training set and validation set.

```{r predictivemodeling0}
set.seed(1000)

### Split into training and validation dataset
inTrain <- createDataPartition(set.training$classe, p=0.75)[[1]]
training <- set.training[inTrain,]
validation <- set.training[-inTrain,]
```

To prevent the overfitting of the model (it can reduce the accuracy) k-folds cross validation is used.

```{r predictivemodeling1}
### 5-fold cross validation
control <- trainControl(method='cv', number=5)
```

### Decision Tree 

```{r decisiontree0}
### Load rpart package
library(rpart)

### Train the model using decision tree
model.rpart <- train(classe ~ ., method='rpart',
                     trControl=control, data=training)
```

Now the decision tree model is used to make the prediction and its accuracy is computed with the confusion matrix.

```{r decisiontree1}
### Predict the outcome
predict.rpart <- predict(model.rpart, validation)

### Determine the accuracy
confusionMatrix(predict.rpart, validation$classe)$overall[1]
```

### Random Forest 

```{r randomforest0}
### Load randomForest package
library(randomForest)

### Train the model using random forest
model.rf <- randomForest(classe ~ ., data=training)
```

Now the random forest model is used to make the prediction and its accuracy is computed with the confusion matrix.

```{r randomforest1}
### Predict the outcome
predict.rf <- predict(model.rf, validation)

### Determine the accuracy
confusionMatrix(predict.rf, validation$classe)$overall[1]
```

### Gradient Boost 

```{r gradientboost0}
### Train the model using gradient boost
gbm.out <- capture.output(
  model.gbm <- train(classe ~ ., method='gbm',
                       trControl=control, data=training)
)
rm(gbm.out)
```

Now the gradient boost model is used to make the prediction and its accuracy is computed with the confusion matrix.

```{r gradientboost1}
### Predict the outcome
predict.gbm <- predict(model.gbm, validation)

### Determine the accuracy
confusionMatrix(predict.gbm, validation$classe)$overall[1]
```

The values of accuracy reported above clearly show how the Random Forest algorithm is the most accurate model. 

## Final Model Testing

The Random Forest predictive model is the more suited to use as final model.

```{r finalmodeltest}
### Predict the outcome on the testing set
predict.final <- predict(model.rf, set.testing)

### Show the final outcomes
predict.final
```